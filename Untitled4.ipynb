{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFsxuSuCe914Or7ZJv7kjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amritsinchan/predictions_ml/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rSIwKxDFIJC"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam, Adadelta\n",
        "from keras.models import load_model\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Generic imports\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np, string, pickle, warnings, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0c-T7tZGnPu"
      },
      "source": [
        "topWords = 50000\n",
        "MAX_LENGTH = 200\n",
        "nb_classes = 2\n",
        "\n",
        "# Downloading data\n",
        "imdbData = imdb.load_data(path='imdb.npz', num_words=topWords)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdbData"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhB9_gNVGrs6",
        "outputId": "80a10706-9f03-436f-e0d2-880ee7457f9e"
      },
      "source": [
        "stopWords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \\\n",
        "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \\\n",
        "             'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \\\n",
        "             'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', \\\n",
        "             'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "             'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', \\\n",
        "             'above', 'below', 'to', 'from', 'off', 'over', 'then', 'here', 'there', 'when', 'where', 'why', \\\n",
        "             'how', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'own', 'same', 'so', \\\n",
        "             'than', 'too', 's', 't', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "             've', 'y', 'ma']\n",
        "word2Index = imdb.get_word_index()\n",
        "index2Word = {v: k for k, v in word2Index.items()}\n",
        "index2Word[0] = \"\"\n",
        "sentimentDict = {0: 'Negative', 1: 'Positive'}\n",
        "\n",
        "def getWordsFromIndexList(indexList):\n",
        "    wordList = []\n",
        "    for index in indexList:\n",
        "      if index in index2Word:\n",
        "        wordList.append(index2Word[index])\n",
        "\n",
        "    return \" \".join(wordList)\n",
        "\n",
        "def getSentiment(predictArray):\n",
        "    pred = int(predictArray[0])\n",
        "    return sentimentDict[pred]\n",
        "\n",
        "def getIndexFromWordList(wordList):\n",
        "    indexList = []\n",
        "    for word in wordList:\n",
        "      if word in word2Index:\n",
        "        indexList.append(str(word2Index[word]))\n",
        "        \n",
        "    return indexList"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbGG1pwaGygl",
        "outputId": "6eb7f40e-4f0a-4b4e-ef0f-8a663c6b3621"
      },
      "source": [
        "print (len(word2Index))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6xC10isG1nj",
        "outputId": "28c779da-40e2-432d-bf1e-21c81fc70157"
      },
      "source": [
        "print(getWordsFromIndexList(x_train[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9ev5RG0G6-X",
        "outputId": "bd5f2d92-fe78-472f-c648-fdd819b24503"
      },
      "source": [
        "print(len(x_train[0]), x_train[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "218 [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRtd286aG76f"
      },
      "source": [
        "stopIndexList = []\n",
        "for stopWord in stopWords:\n",
        "    stopIndexList.append(word2Index[stopWord])\n",
        "\n",
        "trainData = []\n",
        "\n",
        "for indexList in x_train:\n",
        "    processedList = [index for index in indexList if index not in stopIndexList]\n",
        "    trainData.append(processedList)\n",
        "    \n",
        "x_train = trainData"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ5wT0sRG_VP"
      },
      "source": [
        "'''Padding data to keep vectors of same size\n",
        "If size < 200 then it will be padded, else it will be cropped\n",
        "'''\n",
        "trainX = pad_sequences(x_train, maxlen = MAX_LENGTH, padding='post', value = 0.)\n",
        "testX = pad_sequences(x_test, maxlen = MAX_LENGTH, padding='post', value = 0.)\n",
        "\n",
        "'''\n",
        "One-hot encoding for the classes\n",
        "'''\n",
        "trainY = np_utils.to_categorical(y_train, num_classes = nb_classes)\n",
        "testY = np_utils.to_categorical(y_test, num_classes = nb_classes)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kgssy9wHDoE",
        "outputId": "ea220c5e-f775-4c58-e37a-de12bece89de"
      },
      "source": [
        "print(len(trainX[0]), trainX[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200 [   43   973  1622  1385   458  4468  3941   173   256    43   838   112\n",
            "   670 22665   480   284   150   172   112   167 21631   336   385   172\n",
            "  4536  1111    17   546   447   192  2025    19  1920  4613   469    43\n",
            "    76  1247    17   515    17   626 19193    62   386     8   316     8\n",
            "   106  2223  5244   480  3785   619  1415   215    28    52 10311     8\n",
            "   107  5952   256 31050     7  3766   723    43   476   400   317     7\n",
            " 12118  1029   104   381   297  2071   194  7486   226    21   476   480\n",
            "   144  5535    28   224   104   226  1334   283  4472   113   103  5345\n",
            "    19   178     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVBhzIpPHT6_"
      },
      "source": [
        "sgdOptimizer = 'adam'\n",
        "lossFun='categorical_crossentropy'\n",
        "batchSize=1024\n",
        "numEpochs = 50\n",
        "numHiddenNodes = 128\n",
        "EMBEDDING_SIZE = 300\n",
        "denseLayer1Size = 256\n",
        "denseLayer2Size = 128"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH6b5UWAHYww",
        "outputId": "54b0cb08-743e-4e21-fe0c-193c9dd27c21"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Train Embedding layer with Embedding Size = 300\n",
        "model.add(Embedding(topWords, EMBEDDING_SIZE, input_length=MAX_LENGTH, mask_zero=True, name='embedding_layer'))\n",
        "\n",
        "# Define Deep Learning layer\n",
        "model.add(Bidirectional(LSTM(numHiddenNodes), merge_mode='concat',name='bidi_lstm_layer'))\n",
        "\n",
        "# Define Dense layers\n",
        "model.add(Dense(denseLayer1Size, activation='relu', name='dense_1'))\n",
        "model.add(Dropout(0.25, name = 'dropout'))\n",
        "model.add(Dense(denseLayer2Size, activation='relu', name='dense_2'))\n",
        "\n",
        "# Define Output Layer\n",
        "model.add(Dense(nb_classes, activation='softmax', name='output'))\n",
        "\n",
        "model.compile(loss=lossFun, optimizer=sgdOptimizer, metrics=[\"accuracy\"])\n",
        "print(model.summary())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_layer (Embedding)  (None, 200, 300)          15000000  \n",
            "_________________________________________________________________\n",
            "bidi_lstm_layer (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 15,538,242\n",
            "Trainable params: 15,538,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xNTH9uBHeNl",
        "outputId": "9e6d4cd0-d8e8-4873-8531-a90c30e8ec78"
      },
      "source": [
        "model.fit(trainX, trainY, batch_size=batchSize, epochs=numEpochs, verbose=1, validation_data=(testX, testY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2/25 [=>............................] - ETA: 2:43 - loss: 0.6292 - accuracy: 0.7563"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW5x0mySHk48"
      },
      "source": [
        "score = model.evaluate(testX, testY, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWUMixL2HnkS"
      },
      "source": [
        "predY = model.predict_classes(testX)\n",
        "yPred = np_utils.to_categorical(predY, num_classes = nb_classes)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(testY, yPred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98sW3NwwHqVI"
      },
      "source": [
        "model.save('imdb_bi_lstm_tensorflow_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58-J6GzgHvHW"
      },
      "source": [
        "loaded_model = load_model('imdb_bi_lstm_tensorflow_model.hdf5')\n",
        "print(loaded_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2QYM2A0Hyev"
      },
      "source": [
        "num = 121\n",
        "num_next = num + 1\n",
        "print(\"Testing for test case...\" + str(num))\n",
        "groundTruth = testY[num]\n",
        "\n",
        "sampleX = testX[num:num_next]\n",
        "predictionClass = loaded_model.predict_classes(sampleX, verbose=0)\n",
        "prediction = np_utils.to_categorical(predictionClass, num_classes = nb_classes)[0]\n",
        "\n",
        "print(\"Text: \" + str(getWordsFromIndexList(x_test[num-1])))\n",
        "print(\"\\nPrediction: \" + str(getSentiment(predictionClass)))\n",
        "if np.array_equal(groundTruth,prediction):\n",
        "    print(\"\\nPrediction is Correct\")\n",
        "else:\n",
        "    print(\"\\nPrediction is Incorrect\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}